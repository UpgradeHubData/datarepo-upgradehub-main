{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introducción a la estadística para dummies (Ejercicios).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ejercicio 1: Algoritmo *Naive Bayes*\n","\n","Este algoritmo se caracteriza por ser extremadamente rápido de computar, y sus aplicaciones se extienden a numerosos campos de la inteligencia artificial. Sin embargo, cuenta con un gran inconveniente (presente en la hipótesis del teorema de Bayes), y es que las *features* se asumen independientes, lo cual es muy poco frecuente en la vida real (aunque puede conseguirse mediante técnicas como *PCA*).\n","\n","El teorema de Bayes nos da una forma de calcular probabilidades de eventos *a posteriori* conociendo la información *a priori*:\n","\n","$$ P(c|x)=\\frac{P(x|c) P(c)}{P(x)} $$\n","\n","En términos relacionados a nuestro contexto, podría leerse como sigue: *la probabilidad de una determinada clase dada una serie de features, es igual al producto de la probabilidad de dichas features dada una clase, por la probabilidad de dicha clase y el inverso de las features*.\n","\n","Nuestro objetivo en este ejercicio será replicar el algoritmo de *Naive Bayes* con una implementación que mimetice a la de la famosa librería *scikit-learn*. Para ello, dado un cierto *array* de *features* y otro *array* con las etiquetas a predecir, debemos de calcular:\n","\n","* Las probabilidades *a priori* en un método llamado `calc_prior` que se almacenen en una variable interna de la clase llamada `prior`.\n","* Las estadísticas de cada grupo de etiquetas (media y varianza) en un método llamado `calc_statistics` que se almacenen en una variable interna de la clase llamada `mean` y `var` respectivamente.\n","* La probabilidad de cada grupo. Este apartado depende de la asunción que hagamos sobre la distribución de nuestros datos; en este caso, para no distraer la atención del ejercicio, lo damos hecho (método `gaussian_density`) y asumiremos que nuestras *features* son todas continuas y se distribuyen de forma normal.\n","* La probabilidad *a posteriori*. Esto también lo damos hecho por simplicidad.\n","* Un método que nos permita sacar estos cálculos de unos determinados datos de entrenamiento. Esto también lo damos hecho por simplicidad.\n","* Un método que nos permita hacer predicciones en datos en los que el modelo no ha sido entrenado. Esto también lo damos hecho por simplicidad.\n","\n","Partimos del conjunto de datos `iris`:"],"metadata":{"id":"hLXDihqv00X8"}},{"cell_type":"code","source":["# Requerimientos\n","import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","# Cargar datos\n","features, target = load_iris(return_X_y = True, as_frame=True)"],"metadata":{"id":"vbRXAUzQ3gsb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Procedamos ahora a la implementación de los dos pequeños métodos de nuestro algoritmo:"],"metadata":{"id":"F1we7mfZ3w2w"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_WKDL6h0e60"},"outputs":[],"source":["class NaiveBayesClassifier():\n","  \n","    def calc_prior(self, features, target):\n","        '''\n","        Método para calcular probabilidades a priori\n","        '''\n","        # ESCRIBE AQUÍ LAS PROBABILIDADES A PRIORI POR ETIQUETA\n","        self.prior = None #(features.groupby(target).apply(lambda x: len(x)) / self.rows).to_numpy()\n","        return self.prior\n","\n","\n","    def calc_statistics(self, features, target):\n","        '''\n","        Calculamos media y varianza de cada columna\n","        '''\n","        # ESCRIBE AQUÍ LAS MÉTRICAS POR ETIQUETA\n","        self.mean = None #features.groupby(target).apply(np.mean).to_numpy()\n","        self.var = None #features.groupby(target).apply(np.var).to_numpy()\n","              \n","        return self.mean, self.var\n","\n","\n","    def gaussian_density(self, class_idx, x):     \n","        '''\n","        calculate probability from gaussian density function (normally distributed)\n","        we will assume that probability of specific target value given specific class is normally distributed\n","        \n","        probability density function derived from wikipedia:\n","        (1/√2pi*σ) * exp((-1/2)*((x-μ)^2)/(2*σ²)), where μ is mean, σ² is variance, σ is quare root of variance (standard deviation)\n","        '''\n","        mean = self.mean[class_idx]\n","        var = self.var[class_idx]\n","        numerator = np.exp((-1/2)*((x-mean)**2) / (2 * var))\n","        denominator = np.sqrt(2 * np.pi * var)\n","        prob = numerator / denominator\n","        return prob\n","\n","\n","    def calc_posterior(self, x):\n","        posteriors = []\n","\n","        # Calculamos la probabilidad posterior de cada clase\n","        for i in range(self.count):\n","            prior = np.log(self.prior[i])\n","            conditional = np.sum(np.log(self.gaussian_density(i, x)))\n","            posterior = prior + conditional\n","            posteriors.append(posterior)\n","        # Devuelve clase con mayor probabilidad a posteriori\n","        return self.classes[np.argmax(posteriors)]\n","\n","\n","    def fit(self, features, target):\n","        self.classes = np.unique(target)\n","        self.count = len(self.classes)\n","        self.feature_nums = features.shape[1]\n","        self.rows = features.shape[0]\n","        \n","        self.calc_statistics(features, target)\n","        self.calc_prior(features, target)\n","\n","\n","    def predict(self, features):\n","        y_pred = np.array([self.calc_posterior(f) for f in features.to_numpy()])\n","        return y_pred"]},{"cell_type":"markdown","source":["A continuación, en esta breve pieza de código podremos brevemente adelantarnos a la parte de modelización del temario para hacer predicciones con este modelo:"],"metadata":{"id":"rYYtxC2z31U2"}},{"cell_type":"code","source":["# Método para estratificar nuestros datos\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","# Dividimos los datos de forma que haya la misma proporción de etiquetas en cada partición\n","train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=.2, stratify=target)\n","# Instanciamos la clase anterior, entrenamos en una partición y predecimos en la otra\n","NBC = NaiveBayesClassifier()\n","NBC.fit(train_features, train_target)\n","predicciones = NBC.predict(test_features)\n","# Vemos cuál ha sido el rendimiento del modelo\n","print(classification_report(test_target, predicciones))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdT1RXVT37KA","executionInfo":{"status":"ok","timestamp":1655372535965,"user_tz":-120,"elapsed":196,"user":{"displayName":"Antonio Zarauz Moreno","userId":"17853094842922296253"}},"outputId":"5b8c77da-fb56-4097-c4de-bc891fde1ee6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       0.82      0.90      0.86        10\n","           2       0.89      0.80      0.84        10\n","\n","    accuracy                           0.90        30\n","   macro avg       0.90      0.90      0.90        30\n","weighted avg       0.90      0.90      0.90        30\n","\n"]}]},{"cell_type":"markdown","source":["# Ejercicio 2: Máxima verosimilitud\n","\n","Realice un estudio similar al de los apuntes para estimar el parámetro $\\lambda$ de una distribución Poisson mediante la máxima verosimilitud. La distribución de Poisson se encuentra en multitud de fenómenos naturales y es por ello fruto de modelización en múltiples experimentos, por lo que su conocimiento es fundamental.\n","\n","Para ello, siga los pasos:\n","\n","* Dado que el dominio de una distribución Poisson son los números enteros no negativos, considere una serie de $n$ experimentos independientes e idénticamente distribuídos donde en cada uno de ellos resulta tener lugar el suceso $\\{n_i\\}_{i=0}^n$. \n","* Dado que son sucesos independientes, la probabilidad conjunta se expresa como producto de probabilidades marginales.\n","* Aplique el logaritmo y simplifique esta expresión.\n","* Utilice derivadas e iguale a cero la expresión para despejar $\\hat{\\lambda}$.\n","\n","Recuerde que la función masa de probabilidad de una distribución $X\\sim \\mathcal{P}(\\lambda)$ es $P[X=x]=\\frac{\\lambda^x\\cdot e^{-x}}{x!}$.\n","\n","<details>\n","<summary>\n","Solución:\n","</summary>\n","\\begin{array}{ccl}\n","\\text{(1)} & &  P(X|\\lambda)=\\prod_{k=1}^n \\frac{\\lambda^{n_k}\\cdot e^{-n_k}}{n_k!}\\\\\n","\\text{(2)} & &  \\log{(P(X|\\lambda))}=\\sum_{k=1}^n [n_k\\cdot\\log{(\\lambda)} - n_k]-n\\cdot (n_k!)=\\left(\\sum_{k=1}^n n_k\\right)\\cdot(\\log{(\\lambda)}-1)-n\\cdot (n_k!)\\\\\n","\\text{(3)} & &  \\frac{\\partial}{\\partial \\lambda}\\log{(P(X|\\lambda))}=\\frac{1}{\\lambda}\\sum_{k=1}^n n_k=0\\\\\n","(4) & & \\hat{\\lambda}=\\sum_{k=1}^n n_k\n","\\end{array}\n","</details>"],"metadata":{"id":"oahTcmZf5S9N"}},{"cell_type":"code","source":[""],"metadata":{"id":"Z2TJmxAv5Upl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ejercicio 3: Valores atípicos\n","\n","Construya una función que, dado un conjunto de variables continuas en forma de columnas en un objeto `pd.DataFrame`, realice los siguientes pasos para cada una de ellas:\n","\n","* Calcule los percentiles 75 y 25.\n","* Calcule el rango intercuartílico.\n","* Calcule el intervalo $ \\left[Q_1-1.5\\cdot\\text{IQR}, Q_3+1.5\\cdot\\text{IQR}\\right] $.\n","* Localice los valores que no estén en este intervalo por ser **inferiores** y reemplácelos por $Q_1-1.5\\cdot\\text{IQR}$.\n","* Localice los valores que no estén en este intervalo por ser **superiores** y reemplácelos por $Q_3+1.5\\cdot\\text{IQR}$.\n","\n","Se valorará adicionalmente a aquellos alumnos que sean capaces de añadir además que se pueda hacer un estudio de valores atípicos agrupando por variables con alta correlación con la variable respuesta.\n","\n","Aplique lo anterior para analizar las variables continuas del siguiente *dataset*:\n","\n","```python\n","# Requerimientos\n","import os\n","import pandas as pd\n","# Creamos carpeta\n","!mkdir /content/air_quality_dataset\n","# Movemos directorio activo\n","%cd /content/air_quality_dataset\n","# Descargamos fichero comprimido\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip\n","# Descargamos el fichero que contiene los datos a nuestro directorio activo\n","!unzip PRSA2017_Data_20130301-20170228.zip\n","# Nos movemos a la carpeta que contenía el zip\n","%cd PRSA_Data_20130301-20170228\n","# Leemos todos los archivos en una línea\n","df = pd.concat([pd.read_csv(elem) for elem in os.listdir()]).reset_index(drop=True)\n","```"],"metadata":{"id":"OfVMZpSL8pwC"}},{"cell_type":"markdown","source":["# Ejercicio 4: Correlación\n","\n","Implemente un método que tenga los siguientes pasos:\n","\n","* Cree una matriz de correlación de las vriables continuas de un *dataset* y lo compare a la variable respuesta.\n","* Si hay dos variables altamente correlacionadas, elimine del *dataset* la variable que menos correlacionada esté con la variable respuesta.\n","\n","Para ello, come como referencia el siguiente *dataset*:\n","\n","```python\n","# Dependencias\n","import pandas as pd\n","# Descargamos el fichero que contiene los datos a nuestro directorio activo\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00514/Bias_correction_ucl.csv\n","# Leemos fichero\n","df = pd.read_csv('Bias_correction_ucl.csv')\n","```\n","\n","Tome como variable respuesta `Next_Tmax`."],"metadata":{"id":"Lf1bSNEBFwE1"}},{"cell_type":"code","source":[""],"metadata":{"id":"p0DSQSMSGtxd"},"execution_count":null,"outputs":[]}]}