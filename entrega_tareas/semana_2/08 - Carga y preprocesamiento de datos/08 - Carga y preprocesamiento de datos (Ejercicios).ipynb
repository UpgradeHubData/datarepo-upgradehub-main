{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07 - Carga y preprocesamiento de datos (Ejercicios).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPiRB+Nc8NXknTDpQir/DVP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 7 - Carga y preprocesamiento de datos (Ejercicios)\n","\n","En este apartado, hemos estudiado uno de los componentes más importantes del ciclo de vida de los modelos de inteligencia artificial: la adquisición y manipulación de los datos. En los conjuntos de datos propuestos, tendremos por una parte la información relativa a los atributos de la muestra, y por otra los nombres de los mismos. Tu tarea será:\n","\n","* Importar desde consola por comandos Linux los ficheros oportunos.\n","* Leer los ficheros `data` y `names`.\n","* Explorar el fichero `names` para analizar qué tipo de expresiones regulares necesitas para identificar los nombres de las columnas en la metadata.\n","* Aplicar las transformaciones *regex* pertinentes y obtener los nombres de las columnas para construir los datos. *Pista: El nombre de la variable respuesta tendremos que añadirlo al final ya que no viene explícitamente citado*.\n","* Realizar un conveniente preprocesmiento de las variables en función de su tipo."],"metadata":{"id":"7_hr06ahXKd7"}},{"cell_type":"markdown","source":["## 7.1 - *Adult* [dataset](https://archive.ics.uci.edu/ml/datasets/Adult)\n","\n"],"metadata":{"id":"vd83apIaZtHH"}},{"cell_type":"code","source":["import os\n","import re\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"ispLepCMZOoG","executionInfo":{"status":"ok","timestamp":1653908518634,"user_tz":-120,"elapsed":737,"user":{"displayName":"Antonio Zarauz Moreno","userId":"17853094842922296253"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CJ2qGh9XBwJ"},"outputs":[],"source":["# Creamos una carpeta para que contenga a nuestro dataset\n","!mkdir /content/adult_dataset\n","# Movemos el directorio activo a esa localización\n","%cd /content/adult_dataset\n","# Descargamos el fichero que contiene los datos a nuestro directorio activo\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n","# Descargamos la metadata asociada al conjunto de datos\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n","# Leemos datos\n","with open(os.path.join(os.getcwd(),'adult.data'),'r') as f:\n","    data = f.read().splitlines() # Dividimos el texto por saltos de línea\n","    data = [elem.split(',') for elem in data] # Dividimos cada línea por las comas y removemos líneas vacías\n","# Leemos metadata\n","with open(os.path.join(os.getcwd(),'adult.names'),'r') as f:\n","    metadata = f.read().splitlines()\n","# Regex\n","regex_fn = lambda text: re.findall('^[a-zA-Z-]+:{1}', text)\n","reg_text_fn = lambda text : re.findall('[a-zA-Z- ]+', text)\n","metadata_list = [regex_fn(elem)[0] for elem in metadata if regex_fn(elem)]\n","col_names = [reg_text_fn(elem)[0] for elem in metadata_list if reg_text_fn(elem)] + ['label']\n","# Construimos el objeto pd.DataFrame\n","df = pd.DataFrame(data=data, columns=col_names)"]},{"cell_type":"markdown","source":["## 7.2 - Beijing Multi-Site [Air Quality Data](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)\n","\n","En este conjunto de datos no tendremos que hacer un esfuerzo muy grande en lo relativo a estudiar la *metadata*, pero exploraremos una serie de comandos de Linux que nos será muy útil conocer:"],"metadata":{"id":"THZ2TxfIcaBE"}},{"cell_type":"code","source":["# Movemos el directorio activo a una nueva localización para este dataset\n","## Retrocedemos un nivel\n","%cd ..\n","## Creamos carpeta\n","!mkdir /content/air_quality_dataset\n","## Movemos directorio activo\n","%cd /content/air_quality_dataset\n","# Descargamos fichero comprimido\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip\n","# Descargamos el fichero que contiene los datos a nuestro directorio activo\n","!unzip PRSA2017_Data_20130301-20170228.zip\n","# Nos movemos a la carpeta que contenía el zip\n","%cd PRSA_Data_20130301-20170228"],"metadata":{"id":"bra5pAQQc1Pf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora te toca, ¿eres capaz de leer todos los `csv`, concatenarlos y construir un `pd.DataFrame` en una sola línea de código?\n","\n","\n","```python\n","df = pd.concat([pd.read_csv(elem) for elem in os.listdir()]).reset_index(drop=True)\n","```"],"metadata":{"id":"WavKUb55hd_2"}},{"cell_type":"code","source":[""],"metadata":{"id":"gnPYC-ySo3u-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7.3 - Solar flare [dataset](https://archive.ics.uci.edu/ml/datasets/Solar+Flare)\n","\n","En este conjunto de datos, tendremos dos ficheros relativos a `data`, cuya primera fila serán las especificaciones temporales, por lo que deberemos quitarla, y además en los registros de datos las variables no vienen delimitadas por `','`, si no por espacios en blanco:"],"metadata":{"id":"DH3nPI72jN5z"}},{"cell_type":"code","source":["# Movemos el directorio activo a una nueva localización para este dataset\n","## Retrocedemos dos niveles\n","%cd ..\n","%cd ..\n","## Creamos carpeta\n","!mkdir /content/solar_flare_dataset\n","## Movemos directorio activo\n","%cd /content/solar_flare_dataset\n","# Descargamos los ficheros que contienen los datos a nuestro directorio activo\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.data1\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.data2\n","# Descargamos la metadata asociada al conjunto de datos\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.names\n","# Leemos datos\n","## Leemos primer fichero de datos\n","with open(os.path.join(os.getcwd(),'flare.data1'),'r') as f:\n","    data1 = f.read().splitlines() # Dividimos el texto por saltos de línea\n","    data1 = [elem.split(' ') for elem in data1 if elem!=''] # Dividimos cada línea por las comas y removemos líneas vacías\n","    data1 = data1[1:] # Quitamos la línea de metadata temporal\n","## Leemos segundo fichero de datos\n","with open(os.path.join(os.getcwd(),'flare.data2'),'r') as f:\n","    data2 = f.read().splitlines() # Dividimos el texto por saltos de línea\n","    data2 = [elem.split(' ') for elem in data2 if elem!=''] # Dividimos cada línea por las comas y removemos líneas vacías\n","    data2 = data2[1:] # Quitamos la línea de metadata temporal\n","## Combinamos ambas listas\n","data = data1+data2\n","# Leemos metadata\n","with open(os.path.join(os.getcwd(),'flare.names'),'r') as f:\n","    metadata = f.read().splitlines()\n","## Regex\n","regex_fn = lambda text: re.findall('^\\s+[0-9]+\\.{1}\\s{1}[a-zA-Z- ]+', text)\n","reg_text_fn = lambda text : re.findall('[a-zA-Z-]+', text)\n","metadata_list = [regex_fn(elem)[0].strip() for elem in metadata if regex_fn(elem)]\n","col_names = [reg_text_fn(elem)[0] for elem in metadata_list if reg_text_fn(elem)]\n","# Construimos el objeto pd.DataFrame\n","df = pd.DataFrame(data=data, columns=col_names)"],"metadata":{"id":"NUEpFJ5mjRlG"},"execution_count":null,"outputs":[]}]}